{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a regression problem with output space $\\gamma = R$. We have seen that when the loss used is the square loss $l_2(y, z) = (y - z)^2$, then the Bayes predictor is the conditional expectation  of Y given x, for each value of $x \\in \\chi$ ($\\chi$ is the input space) :\n",
    "\n",
    "$$f^*(x) = E[Y|X=x]$$\n",
    "\n",
    "The goal of this exercise is to determine $f^*(x)$ where instead of using the square loss $l_2$, we use the absolute loss $l_1(y, z) = |y - z|$.\n",
    "\n",
    "Question 1 (M + C). Propose a setting where the Bayes predictor is different for the square loss and for the absolute loss, and compute the value of both. Run simulations that verify your results by approximating the Bayes risks for both losses with empirical test errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let set $P(Y|X=x)$ where $P$ is an exponential continuous probability distribution $e^{\\lambda}$.   \n",
    ">The Bayes estimator for the absolute loss $l_1$ is the median of $Y|X=x$ so:\n",
    ">$$f_1^*(x)=\\frac{\\ln(2)}{\\lambda}$$\n",
    ">\n",
    ">Let's compute the Bayes estimator for the square loss $l_2$.\n",
    ">$$f_2^*(x) = E[Y|X = x|] = E[\\lambda e^{-\\lambda}] = \\frac{1}{\\lambda}$$\n",
    ">The estimators are not the same, even though they use the same setting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (M). General case : we consider a setting where for each value $x \\in \\chi$, the conditional probability $P(Y|X=x)$ has a continuous density, noted $p_{Y|X=x}$, and that the conditional variable $P(Y|X=x)$ has a moment of order 1. We note that for all $z \\in R$, that implies that $Y - z|X=x$ also has a moment of order 1.\n",
    "\n",
    "Determine the Bayes predictor, which means for a fixed x, determine:\n",
    "\n",
    "<!--- $$f^*(x) = \\mbox{arg min}_{z \\in R} E[|y - z||X=x] = \\mbox{arg min}_{z \\in R}(g(z))$$ --->\n",
    "$$f^*(x) = \\arg \\min_{z \\in R} E[|y - z||X=x] = \\arg \\min_{z \\in R}(g(z))$$\n",
    "\n",
    "with\n",
    "\n",
    "$$g(z) = \\int_{y \\in R} |y - z|p_{Y|X=x}(y) \\,dy$$\n",
    "\n",
    "where $g(z)$ is correctly defined, according to the previous assumptions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$g(z) = \\int_{R} |y - z|p_{Y|X=x}(y)dy$   \n",
    ">$g(z) = \\int_{-\\infty}^{z} (z - y)p_{Y|X=x}(y)dy + \\int_{z}^{+\\infty} (y - z)p_{Y|X=x}(y)dy$   \n",
    ">$g(z) = z\\int_{-\\infty}^{z} p_{Y|X=x}(y)dy - \\int_{-\\infty}^{z} yp_{Y|X=x}(y)dy + \\int_{z}^{+\\infty} yp_{Y|X=x}(y)dy - z\\int_{z}^{+\\infty} p_{Y|X=x}(y)dy$   \n",
    ">$g'(x) = (\\int_{-\\infty}^{z} p_{Y|X=x}(y)dy + zp(z)) - zp(z) + zp(z) - (\\int_{z}^{+\\infty} p_{Y|X=x}(y)dy+zp(z))$   \n",
    ">$g'(x) = \\int_{-\\infty}^{z} p_{Y|X=x}(y)dy - \\int_{z}^{+\\infty} p_{Y|X=x}(y)dy$   \n",
    ">and $\\frac{d}{dz}g(z) = 0 \\Leftrightarrow \\int_{-\\infty}^{z} p_{Y|X=x}(y)dy = \\int_{z}^{+\\infty} p_{Y|X=x}(y)dy \\Leftrightarrow\\int_{z}^{+\\infty} p_{Y|X=x}(y)dy = \\frac{1}{2}$   \n",
    ">$\\frac{d^2}{dz^2}g(z) = 2p(z) > 0$, the second derivative is always positive, so our solution  minimizes $g(z)$.   \n",
    ">We can conclude that the Bayes estimator $f^*(x)$ is equal to the median of p, $\\frac{1}{2}$.   \n",
    ">$$f^*(x)=F^{-1}(\\frac{1}{2})$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
